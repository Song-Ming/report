\chapter{Materials and Methods}
\label{chap:Materials and Methods}

\section{Dataset}
\label{imp:sec:Dataset}

\begin{flushleft}
The main dataset used in this project was collected from a local hospital and consists of 4369 elbow 
radiographs in either the lateral or anterior posterior view. Each radiograph was named according 
to the type of injury present (supracondylar fracture, dislocation, contusion etc). Using this
information, the data was grouped into either injury or normal. The injury set includes radiographs 
with untreated fractures and/or dislocations while the normal set consists of all other radiographs. 
The data was further divided into training, validation, and test sets with a 80/10/10 split as shown in 
Table 3.1.

In addition, the MURA dataset from Stanford Machine Learning Group was used for pretraining the CNN 
models (Rajpurkar et al., 2017). This dataset consists of 40561 radiographs of 7 different bodyparts: elbow, finger, 
forearm, hand, humerus, shoulder and wrist. Each image is labelled as either positive (abnormality) or negative 
(no abnormality). The division of the data into training and validation sets is shown in Table 3.2. 
Although MURA also has elbow images, the criteria for abnormality used in MURA differs from the criteria for injury in 
the main dataset. For example, a contusion would be classified as normal in the main dataset but would be classified as 
abnormal in the MURA dataset. Because of this mismatch, the elbow X-rays in MURA could not be combined with the main dataset 
for the training stage.

% elbow images cannot be combined with dataset, must be used for pretraining only

\begin{table}[h!]
\parbox{.45\linewidth}{
    \centering 
    \begin{tabular}{|c|c|c|c|} 
        \hline 
        Set & Normal & Injury & Both \\ 
        \hline
        Training & 1957 & 1538 & 3495\\
        \hline
        Validation & 251 & 186 & 437 \\
        \hline 
        Test & 244 & 193 & 437 \\
        \hline 
        Total & 2452 & 1917 & 4369 \\
        \hline 
    \end{tabular}
    \caption{Training Set}
    \label{tab:Training Set} 
}
\parbox{.45\linewidth}{
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Set & Normal & Injury & Both \\ 
        \hline 
        Training & 21935 & 14870 & 36805 \\
        \hline 
        Validation & 1533 & 1667 & 3200\\
        \hline 
        Total & 23468 & 16537 & 40005 \\
        \hline 
    \end{tabular}
    \caption{Pretraining Set}
    \label{tab:Pretraining Set}
}
\end{table}

\end{flushleft}

\section{Data Processing}
\label{imp:sec:Data Processing}

\begin{flushleft}
To improve the quality of the radiographs, all of the radiographs underwent preprocessing. Firstly, the images were 
resized according to the model used. For the CNN models, the image size used for pretraining and training was 384 x 384 
and 584 x 584 pixels respectively. As the models are fully convolutional, they could accept any image size without modifications 
to the architecture. For MedGemma and LLaVA-Rad, the images were resized to 896 x 896 and 518 x 518 respectively according
to the requirements for each model.

Next, the radiographs underwent Contrast Limited Adaptive Histogram Equalisation (CLAHE) using OpenCV. Adaptive Histogram 
Equalisation is an image processing technique which calculates histograms of the pixel intensities in different parts of an 
image. The pixel intensities are then redistributed using the histograms to improve the contrast of the image (Reza, 2004). 
The implementation in OpenCV splits the image into a non overlapping grid and applies the technique subject to
a clip limit which limits the amount of redistribution to reduce noise. The specific settings used were a clip 
limit of 2.5 and a grid size of 4. CLAHE is useful for detecting fractures as it increases the contrast between a fracture
and the surrounding bone which improves its visibility.

Next, the radiographs were sharpened further using unsharp masking. First, each image was processed with a 
gaussian filter to produce a blurred image. This blurred image was then subtracted from the original image
to produce a mask which is multiplied by a factor of 2 and added back to the original image. This process sharpens
the image and further increases contrast at the edges. The kernel used for the gaussian filter was 5 x 5 for the MURA dataset 
and 7 x 7 for the main dataset.

Lastly, the data was augmented with a combination of geometric transformations as follows: Random rotation between -60° and 60°,
75\% probability of horizontal flip, scaling by a factor between 0.85 and 1.1 and translation along the x and y axis of between
0.02 and 0.12. Data augmentation artificially expands the training set for supervised learning which can reduce overfitting and improve generalisation to 
new data. Each image in the training set was augmented twice for the target dataset and once for the MURA dataset for a total of 10485 images and 
73610 images respectively. Brightness and contrast transformations are also commonly used in image augmentation but were found to negatively 
impact performance and were thus excluded.

% include examples?
\end{flushleft}
\pagebreak

\section{CNN Models}
\label{imp:sec:CNN Models}

\subsection{Architecture}
\label{imp:sec:Architecture}

\begin{flushleft}
Convolutional Neural Networks (CNNs) are a type of neural network which use convolutional filters or kernels to 
perform convolutions on images and produce intermediate feature maps. These feature maps are then passed onto the following 
convolutional layers. At the final layer, the feature maps are pooled and flattened to produce a feature vector which is 
processed by a fully connected neural network to generate an output. An example of a CNN architecture is shown below.

\begin{figure}[ht] 
    \centering 
    \includegraphics[height=0.3\textheight,keepaspectratio]{VGG16.png}
    \caption{VGG16 architecture | Credits to Kenneth Leung}
    \label{fig:VGG16 archi}
\end{figure}

For this project, 3 CNN models were used: ConvNeXt, EfficientNetV2  and MobileNetV3. ConvNeXt is a modernised ResNet or
Residual Network which is a deep neural network architecture that is widely used in tasks involving medical imaging
due to its high performance (Liu et al., 2022; Xu et al., 2023). EfficientNetV2 is another family of CNNs which are designed
for efficiency and faster training and have been successful in tasks such as identifying breast cancer from histopathology 
slides (Tan and Le, 2021; Hayat et al., 2024). MobileNetV3 is a lightweight CNN architecture which can be run on mobile devices 
due to its small size and low memory consumption and is useful for real time medical applications (Howard et al., 2019).

After training the individual models, an ensemble model was created using feature fusion for the final classification. 
The framework used for the CNN models is outlined in the following pages.

\pagebreak
\begin{itemize}
  \item 1. Transfer Learning \linebreak
  Transfer learning is a machine learning technique where a machine learning model is trained on one 
  task and is then reused for another related task instead of starting again from scratch. This 
  technique not only saves on training time but can also improve performance on the second task, 
  especially if the amount of available training data is limited.
  
  In this case, the CNN models were initialised with weights which had been pretrained on ImageNet 1k.
  ImageNet 1k is a large dataset with over a million RGB radiographs divided into 1000 categories such as
  plants, animals and furniture and is commonly used as a source of transfer learning for image 
  classification tasks. The weights for the models were obtained from Pytorch Hub which is a publicly
  available model repository.

  \item 2. Greyscale vs RGB \linebreak
  ImageNet is a colour dataset with 3 channel (RGB) radiographs. Hence, the models cannot directly accept 
  greyscale radiographs which only have 1 channel. This issue can be solved by duplicating the single 
  channel twice to create a 3 channel image. However, this process is inefficient as it increases
  the memory cost of storing the image as well as the computational cost of processing the image 
  through the model. To solve this, the first convolutional layer of each model was modified by 
  summing the per channel weights of each kernel. This allows the models to directly accept greyscale
  radiographs without affecting the output of the first layer.

  \item 3. Convolutional Block Attention Module \linebreak
  Convolutional Block Attention Module (CBAM) is an attention mechanism for CNN models which can be
  incorporated into an existing architecture and trained together with the rest of the model (Woo et al., 2018). CBAM
  takes in an intermediate feature map and infers a channel attention map and a spatial attention 
  map. The intermediate feature map is then refined using element wise multiplication with the 2 
  attention maps. CBAM has been shown to improve the performance of CNNs on different computer vision 
  tasks while incurring little additional computational cost.

  ConvNeXt was modified with 4 CBAM modules placed after each of the 4 ConvNeXt blocks. EfficientNet
  was modified with a single CBAM module between the FusedMBConv modules and the MBConv modules, as
  well as a second CBAM module after the final convolutional layer. MobileNet was modified with a
  single CBAM module after the final convolutional layer.

  \pagebreak
  \item 4. Ensembling \linebreak
  Ensembling is a commonly used technique in machine learning which combines the output of several 
  different base models to get a single model which outperforms the individual ones. This can be done
  through several methods such as a simple majority vote or training a model on the outputs of the
  individual models. 
  
  In this case, feature fusion was used to combine the outputs of the 3 individual CNN models. This is 
  done by concatenating the flattened feature vectors of each model before they are sent to the fully 
  connected layer. By default, Convnext has 768 output channels while EfficientNet has 1280 output channels 
  and MobileNet has 960 output channels. To ensure each model has equal input to the ensemble, the final 
  convolutional layer of EfficientNet was modified to have 768 output channels and MobileNet had an 
  additional convolutional layer added to reduce the channels from 960 to 768.

  The concatenated feature vector with 2304 features is then normalised and passed to a dropout layer
  which randomly sets 30\% of the features to 0 to mitigate overfitting. The feature vector is then
  used as the input for a small neural network to generate the final prediction.

\pagebreak
\begin{figure}[ht] 
    \centering 
    
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[height=0.92\textheight,keepaspectratio]{Convnext model.png} 
        \caption{Convnext architecture}
        \label{fig:archi 1}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[height=0.92\textheight,keepaspectratio]{EfficientNet model.png}
        \caption{EfficientNet architecture}
        \label{fig:archi 2}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[height=0.92\textheight,keepaspectratio]{MobileNet model.png}
        \caption{MobileNet architecture}
        \label{fig:archi 3}
    \end{subfigure}

    \caption{Model Architecture}
    \label{fig:Model Architecture}
\end{figure}

\end{itemize}

\end{flushleft}

\clearpage
\subsection{CNN Training}
\label{imp:sec:CNN Training}

\begin{flushleft}
As mentioned previously, ImageNet is a dataset of natural objects and is very different from the 
greyscale radiographs in our target dataset. Hence, it is not an ideal source of transfer learning as
the features learnt from ImageNet may not be useful. To address this issue, the MURA dataset was
used for a pretraining step before training on the target dataset.

For pretraining, the Stochastic Gradient Descent (SGD) optimiser was used together with a cosine annealing 
learning rate scheduler with warm restarts. The training hyperparemeters were batch size of 32, max epochs of 35, 
initial learning rate of 1.2e-3, 8e-4 and 1e-4 for ConvNeXt, EfficientNet and MobileNet respectively, momentum of 0.6 
and weight decay of 1e-3. The cosine annealing scheduler was set to T\_0 of 5 and T\_mult of 2 which means the 
learning rate is decayed over 5 epochs in the first cycle, 10 epochs in the second cycle and 20 in the third. 

Following pretraining, the pretrained weights of the 3 models were used for training on the target dataset. 
The hyperparameters for training were largely the same as pretraining. The differences were max epochs of 28,
momentum of 0.8 and 0.7 for ConvNeXt and EfficientNet respectively and T\_0 of 4. Lastly, for the ensemble model, 
the different hyperparameters were max epochs of 12, momentum of 0.8 and initial learning rate of 2e-4 for the
fully connected layers and 5e-5 for the backbone.
\end{flushleft}

\pagebreak
\section{Vision Language Models}
\label{imp:sec:Vision Language Models}

\subsection{Models}
\label{imp:sec:Models}

\begin{flushleft}
Large language models (LLM) are a type of generative deep learning model which use transformers to learn statistical patterns in natural 
language and predict the next word in a sequence by training on massive text datasets. LLMs break down input text into tokens which are 
represented by numerical vectors called embeddings. The self attention mechanism of the transformers generates query, key and value vectors
from the embeddings using trainable weights. The attention score is then computed from the dot product of the query and key vectors and determines
which tokens are more important. This process allows LLMs to capture the relationship between words and generate coherent outputs (Raiaan et al, 2024).

Vision language models (VLM) are multimodal generative models which expand the capabilities of an LLM by combining it
with a vision encoder which is usually a vision transformer model. The vision encoder processes image inputs into an 
embedding vector which has the same dimensions as the embedding generated by the LLM from the text input. The
embeddings are then fused into a single embedding which combines both visual and textual information. This
embedding is then passed on to the remainder of the LLM to generate a text output. This allows VLMs to 
perform tasks involving image and/or text such as image captioning, question answering and image classification (Zhang et al, 2024). 

Foundation models are large models which have been pretrained on a vast amount of data, usually through self supervised learning. 
Foundation models are versatile and can be used for a wide variety of tasks or as a base for furhter finetuning. In the case of VLMs,
the training data includes unlabelled text and images or labelled text-image pairs (Awais et al, 2025). However, the datasets used for general foundation models
usually do not have much medical data which impacts the performance of these models on medical tasks. To address this issue, foundation VLMs have 
been specifically trained on medical datasets for use in tasks such as medical report generation and diagnosis. For this project, the 2 VLMs used 
are MedGemma 4B and LLava-Rad due to their high performance and relatively small size.

MedGemma is a collection of 2 publicly available foundation models created by google and trained for a wide range of medical related tasks 
such as radiology report generation and diagnosis (Sellergren et al., 2025). MedGemma is based on the Gemma 3 VLM and has a 4 billion parameter and 27 billion 
parameter variant. Both use the SigLIP image encoder and have been trained on a large variety of medical data such as chest 
X-rays, histopathology slides and medical question answer pairs. LLaVA-Rad is another publicly available foundation model created by Microsoft 
and focused on analysing chest X-rays and generating reports (Zambrano Chaves et al., 2025). LLaVA-Rad uses the BiomedCLIP image encoder and the Vicuna LLM and is 
trained on a large dataset of chest X-rays with radiology reports. 
\end{flushleft}

\pagebreak
\subsection{VLM Training}
\label{imp:sec:VLM Training}

\begin{flushleft}
Finetuning the parameters of an entire VLM is extremely computationally and memory intensive due to the large size of the model and also carries the risk 
of catastrophic forgetting which is a phenomenon where a model loses performance on old tasks when finetuned for a new task (Goodfellow et al., 2015). 
To mitigate these issues, parameter efficient finetuning (PEFT) strategies are often used instead (Bafghi et al., 2024). Rather than finetuning the entire model, 
only a small number of additional parameters are trained. 

The specific PEFT strategy used here is a technique called Low Rank Adaptation (LoRA) which involves freezing the parameters of a pretrained model and injecting
additional low rank matrices into the targeted layers of the model (Hu et al., 2021). The updates of the original weight matrices from training can be represented by the 
product of a pair of low rank matrices which are much smaller than the original when the rank is small. During training, the original matrices are frozen and only the 
parameters of the low rank matrices are optimised which greatly decreases the memory and time required. For inference, the product of the low rank matrices is added to the 
original weight matrices to obtain the finetuned weights without significantly increasing inference time. This process can be easily reversed by subtraction to obtain the 
original weights allowing the model to swap between LoRA weights to perform different tasks.

As the target dataset consists solely of images, a prompt was added to each image to instruct the model to analyse the radiograph and classify it as either 
A: 'No injury' or B: 'Fracture or dislocation'. The correct answer is either 'No injury' or 'Fracture or dislocation' depending on the class of the image.
For both MedGemma and LLaVA-Rad, the pretrained weights, tokeniser and image processor were downloaded from Huggingface and used as a starting point for finetuning. 
The checkpoint used for LLaVA-Rad was from the alignment step which only tuned the image encoder. This is because the final finetuning step used LoRA instead of 
directly training the image encoder and LLM. To further reduce memory usage, the existing weights of the model were compressed with 4-bit quantisation using the 
bitsandbytes library (Dettmers et al., 2023).

The TRL and PEFT libraries were used in a custom training script for finetuning MedGemma while LLaVA-Rad was tuned using the provided finetuning script from GitHub.
For both models, the weights of the linear layers of the model were targeted for finetuning. The Adam-W adaptive optimiser was used instead of SGD as with the CNN models 
to increase the convergence speed. For MedGemma, the hyperparameters used were batch size of 32 divided into 8 accumulation steps, max epochs of 5, learning rate 
of 8e-5 with linear scheduling, weight decay of 1e-3, max grad norm of 0.03, LoRA rank of 16 and alpha of 32. For LLaVA-Rad, the different hyperparamters were max 
epochs of 4, learning rate of 1e-4, LoRA rank of 32 and alpha of 64. 
\end{flushleft}

